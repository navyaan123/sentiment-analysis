{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure you have these versions of TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "0a5123fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "1b477834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e2e0b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "train=pd.read_table('train.txt', delimiter = ';', header=None, )\n",
    "val=pd.read_table('val.txt', delimiter = ';', header=None, )\n",
    "test=pd.read_table('test.txt', delimiter = ';', header=None, )\n",
    "\n",
    "data = pd.concat([train ,  val , test])\n",
    "data.columns = [\"text\", \"label\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "c44756fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "c3042a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6eac3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess(line):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', line) #leave only characters from a to z\n",
    "    review = review.lower() #lower the text\n",
    "    review = review.split() #turn string into list of words\n",
    "    #apply Stemming \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] #delete stop words like I, and ,OR   review = ' '.join(review)\n",
    "    #trun list into sentences\n",
    "    return \" \".join(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c0b585db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f32480b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data['N_label'] = label_encoder.fit_transform(data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8c8fea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       didnt feel humili\n",
       "1       go feel hopeless damn hope around someon care ...\n",
       "2                    im grab minut post feel greedi wrong\n",
       "3          ever feel nostalg fireplac know still properti\n",
       "4                                            feel grouchi\n",
       "                              ...                        \n",
       "1995    keep feel like someon unkind wrong think get b...\n",
       "1996              im feel littl cranki neg doctor appoint\n",
       "1997                feel use peopl give great feel achiev\n",
       "1998    im feel comfort derbi feel though start step s...\n",
       "1999    feel weird meet w peopl text like dont talk fa...\n",
       "Name: text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "596e2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model by applying Countvectorizer -convert textual data to numerical data\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))#example: the course was long-> [the,the course,the course was,course, course was, course was long,...]\n",
    "\n",
    "data_cv = cv.fit_transform(data['text']).toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "273c3e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "29d54b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test=data_cv,test_cv,train['N_label'],test['N_label']\n",
    "X_train, X_test, y_train, y_test =train_test_split(data_cv, data['N_label'], test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33985ff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\Sentiments Analysis\\Emotion-Detection-from-Text-using-Neural-Netwroks-main\\.ipynb_checkpoints\\Emotion detection from text using neural networks-checkpoint.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Sentiments%20Analysis/Emotion-Detection-from-Text-using-Neural-Netwroks-main/.ipynb_checkpoints/Emotion%20detection%20from%20text%20using%20neural%20networks-checkpoint.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# first neural network with keras tutorial\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Project/Sentiments%20Analysis/Emotion-Detection-from-Text-using-Neural-Netwroks-main/.ipynb_checkpoints/Emotion%20detection%20from%20text%20using%20neural%20networks-checkpoint.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Sentiments%20Analysis/Emotion-Detection-from-Text-using-Neural-Netwroks-main/.ipynb_checkpoints/Emotion%20detection%20from%20text%20using%20neural%20networks-checkpoint.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Sentiments%20Analysis/Emotion-Detection-from-Text-using-Neural-Netwroks-main/.ipynb_checkpoints/Emotion%20detection%20from%20text%20using%20neural%20networks-checkpoint.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# load the dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Sentiments%20Analysis/Emotion-Detection-from-Text-using-Neural-Netwroks-main/.ipynb_checkpoints/Emotion%20detection%20from%20text%20using%20neural%20networks-checkpoint.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# split into input (X) and output (y) variables\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Project/Sentiments%20Analysis/Emotion-Detection-from-Text-using-Neural-Netwroks-main/.ipynb_checkpoints/Emotion%20detection%20from%20text%20using%20neural%20networks-checkpoint.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# define the keras model\u001b[39;00m\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\__init__.py:45\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_column_lib \u001b[39mas\u001b[39;00m feature_column\n\u001b[0;32m     46\u001b[0m \u001b[39m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m module\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"FeatureColumns: tools for ingesting and representing features.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence_feature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:143\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse_tensor \u001b[39mas\u001b[39;00m sparse_tensor_lib\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n\u001b[1;32m--> 143\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m    144\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops\n\u001b[0;32m    145\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m check_ops\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\layers\\base.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# =============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy_tf_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m     18\u001b[0m InputSpec \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mInputSpec\n\u001b[0;32m     20\u001b[0m keras_style_scope \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mkeras_style_scope\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[39m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\keras\\models.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics \u001b[39mas\u001b[39;00m metrics_module\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizer_v1\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m functional\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m sequential\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m training\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m input_spec\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m node \u001b[39mas\u001b[39;00m node_module\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m training \u001b[39mas\u001b[39;00m training_lib\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m training_utils\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaved_model\u001b[39;00m \u001b[39mimport\u001b[39;00m network_serialization\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:54\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m loss_scale_optimizer \u001b[39mas\u001b[39;00m lso\n\u001b[0;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m policy\n\u001b[1;32m---> 54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m hdf5_format\n\u001b[0;32m     55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m save\n\u001b[0;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m saving_utils\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m   \u001b[39mimport\u001b[39;00m \u001b[39mh5py\u001b[39;00m\n\u001b[0;32m     38\u001b[0m   HDF5_OBJECT_HEADER_LIMIT \u001b[39m=\u001b[39m \u001b[39m64512\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Software\\python\\lib\\site-packages\\h5py\\__init__.py:45\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     _warn((\u001b[39m\"\u001b[39m\u001b[39mh5py is running against HDF5 \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m when it was built against \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mthis may cause problems\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     38\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39mversion\u001b[39m.\u001b[39mhdf5_version_tuple),\n\u001b[0;32m     39\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39mversion\u001b[39m.\u001b[39mhdf5_built_version_tuple)\n\u001b[0;32m     40\u001b[0m     ))\n\u001b[0;32m     43\u001b[0m _errors\u001b[39m.\u001b[39msilence_errors()\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_conv\u001b[39;00m \u001b[39mimport\u001b[39;00m register_converters \u001b[39mas\u001b[39;00m _register_converters, \\\n\u001b[0;32m     46\u001b[0m                    unregister_converters \u001b[39mas\u001b[39;00m _unregister_converters\n\u001b[0;32m     47\u001b[0m _register_converters()\n\u001b[0;32m     48\u001b[0m atexit\u001b[39m.\u001b[39mregister(_unregister_converters)\n",
      "File \u001b[1;32mh5py\\_conv.pyx:1\u001b[0m, in \u001b[0;36minit h5py._conv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5t.pyx:1\u001b[0m, in \u001b[0;36minit h5py.h5t\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load the dataset\n",
    "# split into input (X) and output (y) variables\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b6867f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 6ms/step - loss: 0.8040 - accuracy: 0.8504\n",
      "Accuracy: 85.04\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "fee946e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "text='I feel sad'\n",
    "text=preprocess(text)\n",
    "array = cv.transform([text]).toarray()\n",
    "pred = model.predict(array)\n",
    "a=np.argmax(pred, axis=1)\n",
    "label_encoder.inverse_transform(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "5877746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model,'my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "ea79609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(label_encoder, open('encoder.pkl', 'wb'))\n",
    "pickle.dump(cv, open('CountVectorizer.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e40ecd1c13d910812d277f85151e75fbec09ceda5c87b776cca02e0a006aa1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
